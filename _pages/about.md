---
layout: about
title: about
permalink: /
subtitle: <a href='[#](https://ai.ewha.ac.kr/deptai/index.do)'>Dept. of AI, Ewha Womans University</a>. Seoul, Korea.


selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page

announcements:
  enabled: false # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---
<b style="color: green">Multimodal AI Lab @ EWHA</b> focuses on developing robust and generalizable AI models that understand and generate information across multiple modalitiesâ€”vision, language, and audio. 
Our research spans multimodal generation, large audiovisual/vision-language models, video understanding, 3D perception, and cross-modal grounding. 
We build multimodal systems that learn with minimal supervision and perform reliably in diverse, real-world settings.
We aim to push the boundaries of multimodal learning and create AGI that is creative, effective, and efficient.


**Recruiting Undergraduate Interns/ Graduate Students**:
We are looking for undergraduate interns, and graduate students to collaborate with! 
If you are interested in doing cool multimodal learning research, please send your CV and GPA to <a href="mailto:lee.jiyoung@ewha.ac.kr"><i class="fa-solid fa-envelope"></i></a>.

&nbsp;


### News
<div class="news">
  <div class="table-responsive" style="max-height: 60vw">
  <table class="table table-sm table-borderless">
    <tr>
        <th scope="row" style="width: 20%"> Apr 2025 </th>
        <td> <b style="color: green">Multimodal AI Lab @ EWHA</b> website is now open! ðŸ‘‹ </td>
    </tr>
    <tr>
        <th scope="row" style="width: 20%"> Feb 2025 </th>
        <td> One paper is accepted at <b>CVPR</b> 2025! ðŸŽ‰ </td>
    </tr>
    <tr>
        <th scope="row" style="width: 20%"> Dec 2024 </th>
        <td> Prof.Jiyoung Lee presented at <b><a href="https://event-us.kr/eventinfo/event/96012" style="color: green">Postech AI day</a></b> (topic: Read, Watch and Scream! Sound Generation from Text and Video). </td>
    </tr>
  </table>
  </div>
</div>

**Apr 2025** Multimodal AI Lab @ EWHA website is now open! ðŸ‘‹

**Feb 2025** 1 paper is accepted at **CVPR** 2025.

**Dec 2024** Prof.Jiyoung Lee presented at **<a href="https://event-us.kr/eventinfo/event/96012" style="color: green">Postech AI day</a>** (topic: Read, Watch and Scream! Sound Generation from Text and Video).

**Dec 2024** Prof.Jiyoung Lee presented at **HUST, Vietnam** (topic: Audio Generation from Visual Contents).

**Dec 2024** 1 paper is accepted at **AAAI** 2025.

**Oct 2024** 1 paper is accepted at **NeurIPS 2024 Workshop on Video-Language Models**.

**Sep 2024** 1 paper is accepted in **IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) [Q1]**.

**Sep 2024** Prof.Jiyoung Lee serves a lecture, **<a href="https://naver-ai.github.io/202402-AI773/" style="color: green">Topics in Artificial Intelligence: Multimodal Deep Learning Theories and Applications</a>**, at Seoul National University (Fall 2024).

**Jun 2024** 1 paper is accepted in **Pattern Recognition (PR) [Q1]**.

**Jan 2024** 2 papers are accepted at **ICLR** 2024.

<details>
<summary>ðŸ“‚ older news</summary>
<div markdown="1">
  
**Sep 2023** Prof.Jiyoung Lee serves a lecture, **<a href="https://naver-ai.github.io/202302-AI773/" style="color: green">Topics in Artificial Intelligence: Multimodal Deep Learning Theories and Applications</a>**, at Seoul National University (Fall 2023).

**Jul 2023** 2 papers are accepted at **ICCV** 2023.

**Apr 2023** 1 paper is accepted at **ICML** 2023.

**Apr 2023** 1 paper is accepted at **CVPR Workshop** 2023.

**Feb 2023** 1 paper is accepted at **CVPR** 2023.

**Feb 2023** 1 paper is accepted at **ICASSP** 2023.

**Nov 2022** 1 paper is accepted at **AAAI** 2023.

**Oct 2022** 1 paper is accepted at **WACV** 2023.

**Sep 2022** 1 paper is accepted at **NeurIPS** 2022.

**Jul 2022** 1 paper is accepted at **ECCV** 2022.

**Mar 2022** 2 papers are accepted at **CVPR** 2022.

**Jan 2022** 1 paper is accepted at **ICASSP** 2022.

**Jan 2022** 1 paper is accepted at **CLeaR** 2022.

**Oct 2021** 1 paper is accepted at **BMVC** 2021.

**May 2021** 1 paper is accepted at **ICIP** 2021.

**Mar 2021** 2 papers are accepted at **CVPR** 2021.

**Jul 2020** 1 paper is accepted at **ECCV** 2020.

**May 2020** 1 paper is accepted in **IEEE Transactions on Image Processing (TIP) [Q1]**.

</div>
</details>
